from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import asyncio
import os
import json
from datetime import datetime
import requests
import sys
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.tools import DuckDuckGoSearchRun

router = APIRouter()

# ============ å®‰å…¨è™•ç†å‡½æ•¸ ============
def safe_round(value, decimals=2):
    """å®‰å…¨åœ°é€²è¡Œå››æ¨äº”å…¥"""
    if value is None:
        return 0.0
    try:
        return round(float(value), decimals)
    except (ValueError, TypeError):
        return 0.0

def safe_float(value):
    """å®‰å…¨åœ°è½‰æ›ç‚º float"""
    if value is None:
        return 0.0
    try:
        return float(value)
    except (ValueError, TypeError):
        return 0.0
# =====================================

# é…ç½®
DB_DIR = "./vectordb"
DOCS_DIR = "./docs"
OLLAMA_HOST = "http://localhost:11434"  # Ollama é è¨­åœ°å€

# RAG é…ç½®
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
CHUNK_SIZE = 400
CHUNK_OVERLAP = 100

# åˆå§‹åŒ– DuckDuckGo æœå°‹
def init_duckduckgo_search():
    """åˆå§‹åŒ– DuckDuckGo æœå°‹å·¥å…·"""
    try:
        search_tool = DuckDuckGoSearchRun()
        # æ¸¬è©¦æœå°‹
        test_result = search_tool.run("test")[:100]
        print(f"âœ… DuckDuckGo æœå°‹å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        return search_tool
    except Exception as e:
        print(f"âŒ DuckDuckGo æœå°‹å·¥å…·åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

# åˆå§‹åŒ– RAG å‘é‡è³‡æ–™åº«
def init_rag_vector_db():
    """åˆå§‹åŒ– RAG å‘é‡è³‡æ–™åº«"""
    try:
        if not os.path.exists(DB_DIR):
            print(f"âŒ å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {DB_DIR}")
            return None

        # è¼‰å…¥åµŒå…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        vectordb = Chroma(
            persist_directory=DB_DIR,
            embedding_function=embeddings
        )

        # æ¸¬è©¦è³‡æ–™åº«
        test_docs = vectordb.similarity_search("æ¸¬è©¦", k=1)
        print(f"âœ… RAG å‘é‡è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸï¼Œæ–‡ä»¶æ•¸: {vectordb._collection.count()}")
        return vectordb
    except Exception as e:
        print(f"âŒ RAG å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

# åˆå§‹åŒ–æœå°‹å·¥å…·å’Œå‘é‡è³‡æ–™åº«
DUCKDUCKGO_SEARCH = init_duckduckgo_search()
RAG_VECTORDB = init_rag_vector_db()

class QuestionRequest(BaseModel):
    question: str
    type: str = "rag"  # rag, web, hybrid
    options: Optional[Dict[str, Any]] = None

class QuestionResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]] = []
    metadata: Dict[str, Any] = {}

# æª¢æŸ¥ Ollama æ˜¯å¦å¯ç”¨
def check_ollama_available():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama é€£æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {[m['name'] for m in models]}")
            return True, models
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
        print(f"   è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ: ollama serve")
    return False, []

# åˆå§‹åŒ–æ™‚æª¢æŸ¥ Ollama
OLLAMA_AVAILABLE, AVAILABLE_MODELS = check_ollama_available()

# é è¨­ä½¿ç”¨ DeepSeek R1ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å…¶ä»–å¯ç”¨æ¨¡å‹
def get_preferred_model():
    """ç²å–é¦–é¸æ¨¡å‹"""
    preferred_models = [
        "deepseek-r1:8b",
        "qwen2.5:7b",
        "llama3.2:3b",
        "mistral:7b",
        "gemma:2b"
    ]

    for model in preferred_models:
        for available in AVAILABLE_MODELS:
            if model in available.get('name', ''):
                print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model}")
                return model

    if AVAILABLE_MODELS:
        first_model = AVAILABLE_MODELS[0].get('name', '')
        print(f"âœ… ä½¿ç”¨å¯ç”¨æ¨¡å‹: {first_model}")
        return first_model

    print("âš ï¸  æ²’æœ‰å¯ç”¨çš„ Ollama æ¨¡å‹")
    return None

PREFERRED_MODEL = get_preferred_model()

async def call_ollama_api(prompt: str, model: str = None) -> str:
    """èª¿ç”¨ Ollama API ç”Ÿæˆå›ç­”"""
    if not OLLAMA_AVAILABLE or not PREFERRED_MODEL:
        return "âš ï¸ Ollama æœå‹™æœªé€£æ¥ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚"

    model_to_use = model or PREFERRED_MODEL

    try:
        payload = {
            "model": model_to_use,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 2000
            }
        }

        start_time = datetime.now()
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=payload,
            timeout=300
        )

        if response.status_code == 200:
            result = response.json()
            processing_time = (datetime.now() - start_time).total_seconds()

            answer = result.get("response", "").strip()

            # æ¸…ç†å›ç­”
            if answer.startswith("ã€‚"):
                answer = answer[1:]
            if answer.startswith("ï¼Œ"):
                answer = answer[1:]

            print(f"âœ… Ollama å›ç­”ç”ŸæˆæˆåŠŸï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            return answer, processing_time
        else:
            return f"âŒ Ollama API éŒ¯èª¤: {response.status_code}", 0

    except requests.exceptions.Timeout:
        return "âŒ Ollama è«‹æ±‚è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", 0
    except Exception as e:
        return f"âŒ Ollama èª¿ç”¨å¤±æ•—: {str(e)}", 0

# DuckDuckGo æœå°‹å‡½æ•¸
async def search_duckduckgo(query: str, max_results: int = 5) -> Dict[str, Any]:
    """ä½¿ç”¨ DuckDuckGo é€²è¡Œæœå°‹"""
    if not DUCKDUCKGO_SEARCH:
        return {
            "status": "error",
            "message": "DuckDuckGo æœå°‹å·¥å…·æœªåˆå§‹åŒ–",
            "results": [],
            "query": query
        }

    try:
        print(f"ğŸ” DuckDuckGo æœå°‹: {query}")

        # åŸ·è¡Œæœå°‹
        search_result = DUCKDUCKGO_SEARCH.run(query)

        # è§£ææœå°‹çµæœï¼ˆDuckDuckGoSearchRun è¿”å›çš„æ˜¯æ–‡æœ¬ï¼Œéœ€è¦è§£æï¼‰
        results = []

        if search_result:
            # å°‡æœå°‹çµæœåˆ†å‰²æˆæ®µè½
            paragraphs = search_result.split('\n\n')
            for i, paragraph in enumerate(paragraphs[:max_results]):
                if paragraph.strip():
                    results.append({
                        "index": i + 1,
                        "content": paragraph.strip()[:500],  # é™åˆ¶é•·åº¦
                        "relevance": 1.0 - (i * 0.1),  # ç°¡å–®ç›¸é—œæ€§è©•åˆ†
                        "type": "web_search"
                    })

        print(f"âœ… æ‰¾åˆ° {len(results)} å€‹æœå°‹çµæœ")

        return {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœ",
            "results": results,
            "query": query,
            "search_engine": "DuckDuckGo"
        }

    except Exception as e:
        print(f"âŒ DuckDuckGo æœå°‹å¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e),
            "results": [],
            "query": query
        }

# RAG æª¢ç´¢å‡½æ•¸
async def search_rag(query: str, k: int = 4) -> Dict[str, Any]:
    """ä½¿ç”¨ RAG æª¢ç´¢æœ¬åœ°çŸ¥è­˜åº«"""
    if not RAG_VECTORDB:
        return {
            "status": "error",
            "message": "RAG å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–",
            "results": [],
            "query": query
        }

    try:
        print(f"ğŸ“š RAG æª¢ç´¢: {query}")

        # åŸ·è¡Œç›¸ä¼¼åº¦æœå°‹
        docs = RAG_VECTORDB.similarity_search(query, k=k)

        results = []
        for i, doc in enumerate(docs):
            content = doc.page_content[:400]  # é™åˆ¶é•·åº¦
            source = doc.metadata.get('source', 'æœªçŸ¥')
            results.append({
                "index": i + 1,
                "content": content,
                "source": source,
                "relevance": 1.0 - (i * 0.15),  # ç°¡å–®ç›¸é—œæ€§è©•åˆ†
                "type": "rag_document"
            })

        print(f"âœ… æ‰¾åˆ° {len(results)} å€‹æœ¬åœ°çŸ¥è­˜åº«çµæœ")

        return {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(results)} å€‹æœ¬åœ°çŸ¥è­˜åº«çµæœ",
            "results": results,
            "query": query
        }

    except Exception as e:
        print(f"âŒ RAG æª¢ç´¢å¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e),
            "results": [],
            "query": query
        }

async def rag_qa_internal(question: str, k: int = 4) -> QuestionResponse:
    """RAG å•ç­”å…§éƒ¨å¯¦ç¾ï¼ˆä½¿ç”¨æœ¬åœ°çŸ¥è­˜åº«ï¼‰"""
    start_time = datetime.now()

    try:
        # å…ˆæª¢ç´¢æœ¬åœ°çŸ¥è­˜åº«
        rag_results = await search_rag(question, k)

        # æ§‹å»ºä¸Šä¸‹æ–‡
        if rag_results["status"] == "success" and rag_results["results"]:
            context = "ã€æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šã€‘\n\n"
            for result in rag_results["results"]:
                context += f"ä¾†æº: {result['source']}\n"
                context += f"å…§å®¹: {result['content']}\n\n"
        else:
            context = "æœ¬åœ°çŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"

        # æ§‹å»ºæç¤ºè©
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹æ ¹æ“šæœ¬åœ°çŸ¥è­˜åº«æä¾›ï¼š
1. æº–ç¢ºã€æœ‰ç”¨çš„è³‡è¨Š
2. å…·é«”çš„ç´°ç¯€å’Œæ•¸æ“š
3. å¯¦ç”¨çš„å»ºè­°
4. ç›¸é—œçš„æ³¨æ„äº‹é …

å¦‚æœçŸ¥è­˜åº«ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šï¼Œè«‹åŸºæ–¼æ‚¨çš„çŸ¥è­˜è£œå……èªªæ˜ã€‚
ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¿æŒå°ˆæ¥­ä¸”æ˜“æ–¼ç†è§£ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama
        answer, llm_time = await call_ollama_api(prompt)

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_time = (datetime.now() - start_time).total_seconds()

        # æ§‹å»ºä¾†æºè³‡è¨Š
        sources = []
        if rag_results["status"] == "success" and rag_results["results"]:
            for result in rag_results["results"]:
                sources.append({
                    "source": f"æœ¬åœ°çŸ¥è­˜åº«: {result['source']}",
                    "relevance": result["relevance"],
                    "type": "rag",
                    "content_preview": result["content"][:100]
                })
        else:
            sources.append({
                "source": "AI çŸ¥è­˜åº«",
                "relevance": 0.9,
                "type": "ai"
            })

        sources.append({
            "source": "Ollama AI åˆ†æ",
            "relevance": 0.95,
            "type": "ai",
            "model": PREFERRED_MODEL
        })

        return QuestionResponse(
            answer=answer,
            sources=sources,
            metadata={
                "type": "rag",
                "model_used": PREFERRED_MODEL or "ai_model",
                "ollama_available": OLLAMA_AVAILABLE,
                "processing_time": safe_round(total_time, 2),
                "llm_time": safe_round(llm_time, 2),
                "rag_results_count": len(rag_results["results"]),
                "rag_status": rag_results["status"],
                "answer_source": "ollama_ai_with_rag"
            }
        )

    except Exception as e:
        error_time = (datetime.now() - start_time).total_seconds()
        return QuestionResponse(
            answer=f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)[:100]}",
            sources=[],
            metadata={
                "type": "rag",
                "error": str(e),
                "processing_time": safe_round(error_time, 2)
            }
        )

async def web_qa_internal(question: str) -> QuestionResponse:
    """ç¶²è·¯å•ç­”å…§éƒ¨å¯¦ç¾ï¼ˆä½¿ç”¨ DuckDuckGo æœå°‹ï¼‰"""
    start_time = datetime.now()

    try:
        # ä½¿ç”¨ DuckDuckGo é€²è¡Œæœå°‹
        search_results = await search_duckduckgo(question, max_results=5)

        # æ§‹å»ºæœå°‹çµæœä¸Šä¸‹æ–‡
        if search_results["status"] == "success" and search_results["results"]:
            search_context = "ã€ç¶²è·¯æœå°‹çµæœã€‘\n\n"
            for i, result in enumerate(search_results["results"], 1):
                content = result["content"]
                search_context += f"{i}. {content}\n\n"

            print(f"âœ… ä½¿ç”¨ {len(search_results['results'])} å€‹æœå°‹çµæœ")
        else:
            # å¦‚æœæœå°‹å¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ“¬æœå°‹
            search_context = f"æœå°‹é—œéµå­—ï¼š{question}\n\næœå°‹çµæœï¼š\n1. ç›¸é—œç¶²è·¯è³‡è¨Š\n2. æ–°èå ±å°\n3. ç”¨æˆ¶è¨è«–\n4. å®˜æ–¹è³‡è¨Š"
            print("âš ï¸ ä½¿ç”¨æ¨¡æ“¬æœå°‹çµæœ")

        # æ§‹å»ºçµ¦ Ollama çš„æç¤ºè©
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æœå°‹çµæœå›ç­”å•é¡Œï¼š

å•é¡Œï¼š{question}

{search_context}

è«‹åŸºæ–¼æœå°‹çµæœæä¾›ï¼š
1. é—œéµè³‡è¨Šæ‘˜è¦
2. å¯¦ç”¨å»ºè­°
3. é€²ä¸€æ­¥æŸ¥è©¢çš„æ–¹å‘

ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè¨»æ˜è³‡è¨Šä¾†æºç‚ºç¶²è·¯æœå°‹ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama
        answer, llm_time = await call_ollama_api(prompt)

        total_time = (datetime.now() - start_time).total_seconds()

        # æ§‹å»ºä¾†æºè³‡è¨Š
        sources = []
        if search_results["status"] == "success" and search_results["results"]:
            for result in search_results["results"][:3]:  # åªå–å‰3å€‹
                sources.append({
                    "source": f"DuckDuckGo æœå°‹çµæœ #{result['index']}",
                    "relevance": result["relevance"],
                    "type": "web",
                    "content_preview": result["content"][:100]
                })
        else:
            sources.append({
                "source": "æ¨¡æ“¬ç¶²è·¯æœå°‹",
                "relevance": 0.8,
                "type": "web",
                "note": "å¯¦éš›æœå°‹æœªå•Ÿç”¨æˆ–å¤±æ•—"
            })

        sources.append({
            "source": "Ollama AI åˆ†æ",
            "relevance": 0.9,
            "type": "ai",
            "model": PREFERRED_MODEL
        })

        return QuestionResponse(
            answer=answer,
            sources=sources,
            metadata={
                "type": "web",
                "model_used": PREFERRED_MODEL or "simulation",
                "processing_time": round(total_time, 2),
                "llm_time": round(llm_time, 2) if llm_time else 0,
                "search_engine": search_results.get("search_engine", "simulated"),
                "search_status": search_results["status"],
                "search_results_count": len(search_results["results"])
            }
        )

    except Exception as e:
        return QuestionResponse(
            answer=f"ç¶²è·¯æœå°‹å¤±æ•—ï¼š{str(e)[:100]}",
            sources=[],
            metadata={
                "type": "web",
                "error": str(e),
                "processing_time": round((datetime.now() - start_time).total_seconds(), 2)
            }
        )

async def hybrid_qa_internal(question: str) -> QuestionResponse:
    """æ··åˆå•ç­”å…§éƒ¨å¯¦ç¾ï¼ˆçµåˆ RAG å’Œ Webï¼‰"""
    start_time = datetime.now()

    try:
        print(f"ğŸ”€ é–‹å§‹æ··åˆå•ç­”: {question}")

        # åŒæ™‚é€²è¡Œ RAG æª¢ç´¢å’Œ Web æœå°‹
        rag_task = search_rag(question, k=4)
        web_task = search_duckduckgo(question, max_results=5)

        # ç­‰å¾…å…©å€‹ä»»å‹™å®Œæˆ
        rag_results, web_results = await asyncio.gather(rag_task, web_task)

        # è©•ä¼°å…©ç¨®ä¾†æºçš„ç›¸é—œæ€§
        rag_has_content = rag_results["status"] == "success" and rag_results["results"]
        web_has_content = web_results["status"] == "success" and web_results["results"]

        print(f"ğŸ“Š æª¢ç´¢çµæœ: RAG={len(rag_results['results']) if rag_has_content else 0} å€‹, Web={len(web_results['results']) if web_has_content else 0} å€‹")

        # æ§‹å»ºæ•´åˆçš„ä¸Šä¸‹æ–‡
        context_parts = []

        if rag_has_content:
            rag_context = "ã€æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šã€‘\n\n"
            for result in rag_results["results"]:
                rag_context += f"ä¾†æº: {result['source']}\n"
                rag_context += f"ç›¸é—œæ€§: {result['relevance']:.2f}\n"
                rag_context += f"å…§å®¹: {result['content']}\n\n"
            context_parts.append(rag_context)

        if web_has_content:
            web_context = "ã€ç¶²è·¯æœå°‹è³‡è¨Šã€‘\n\n"
            for i, result in enumerate(web_results["results"], 1):
                web_context += f"çµæœ {i} (ç›¸é—œæ€§: {result['relevance']:.2f}):\n"
                web_context += f"{result['content']}\n\n"
            context_parts.append(web_context)

        if not rag_has_content and not web_has_content:
            context = "âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„æœ¬åœ°çŸ¥è­˜åº«æˆ–ç¶²è·¯è³‡è¨Šã€‚"
        else:
            context = "\n".join(context_parts)

        # æ ¹æ“šå¯ç”¨çš„è³‡è¨Šé¡å‹æ§‹å»ºä¸åŒçš„æç¤ºè©
        if rag_has_content and web_has_content:
            prompt = f"""è«‹ç¶œåˆä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹çš„è³‡è¨Šï¼Œæä¾›ä¸€å€‹å…¨é¢ã€æº–ç¢ºçš„å›ç­”ï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šæä¾›ï¼š
1. æ ¸å¿ƒè³‡è¨Šæ‘˜è¦ï¼ˆç¶œåˆå…©æ–¹é¢è³‡è¨Šï¼‰
2. å…·é«”ç´°ç¯€å’Œæ•¸æ“šï¼ˆå„ªå…ˆä½¿ç”¨æœ¬åœ°çŸ¥è­˜åº«çš„æ¬Šå¨è³‡è¨Šï¼‰
3. å¯¦ç”¨å»ºè­°å’Œæ“ä½œæ­¥é©Ÿ
4. æ³¨æ„äº‹é …å’Œé¢¨éšªæç¤º

å¦‚æœè³‡è¨Šæœ‰è¡çªï¼š
- æŠ€è¡“æ€§ã€å°ˆæ¥­æ€§è³‡è¨Šä»¥æœ¬åœ°çŸ¥è­˜åº«ç‚ºæº–
- æ™‚æ•ˆæ€§ã€æ–°èæ€§è³‡è¨Šä»¥ç¶²è·¯æœå°‹ç‚ºæº–
- è¨»æ˜è³‡è¨Šä¾†æºï¼ˆæœ¬åœ°çŸ¥è­˜åº«/ç¶²è·¯æœå°‹ï¼‰

ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¿æŒå°ˆæ¥­ã€å®¢è§€ä¸”æ˜“æ–¼ç†è§£ã€‚

ç¶œåˆå›ç­”ï¼š"""

        elif rag_has_content:
            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹ä¸»è¦ä½¿ç”¨æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”ï¼Œå¦‚æœè³‡è¨Šä¸è¶³å¯ä»¥è£œå……æ‚¨çš„é€šç”¨çŸ¥è­˜ã€‚
è¨»æ˜è³‡è¨Šä¾†æºç‚ºæœ¬åœ°çŸ¥è­˜åº«ã€‚

å›ç­”ï¼š"""

        elif web_has_content:
            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¶²è·¯æœå°‹çµæœå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹ä¸»è¦ä½¿ç”¨ç¶²è·¯æœå°‹çµæœå›ç­”ï¼Œæ³¨æ„è³‡è¨Šçš„æ™‚æ•ˆæ€§ã€‚
è¨»æ˜è³‡è¨Šä¾†æºç‚ºç¶²è·¯æœå°‹ã€‚

å›ç­”ï¼š"""

        else:
            prompt = f"""è«‹æ ¹æ“šæ‚¨çš„çŸ¥è­˜å›ç­”ä»¥ä¸‹å•é¡Œï¼š

ã€å•é¡Œã€‘
{question}

è«‹æä¾›æº–ç¢ºã€æœ‰ç”¨çš„è³‡è¨Šï¼Œä¸¦è¨»æ˜é€™æ˜¯åŸºæ–¼é€šç”¨çŸ¥è­˜çš„å›ç­”ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama ç”Ÿæˆæ•´åˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç¶œåˆå›ç­”...")
        integrated_answer, llm_time = await call_ollama_api(prompt)

        # åˆä½µä¾†æºè³‡è¨Š
        all_sources = []

        if rag_has_content:
            for result in rag_results["results"][:3]:  # åªå–å‰3å€‹
                all_sources.append({
                    "source": f"æœ¬åœ°çŸ¥è­˜åº«: {result['source']}",
                    "relevance": result["relevance"],
                    "type": "rag",
                    "content_preview": result["content"][:100]
                })

        if web_has_content:
            for result in web_results["results"][:5]:  # åªå–å‰5å€‹
                all_sources.append({
                    "source": f"DuckDuckGo æœå°‹çµæœ #{result['index']}",
                    "relevance": result["relevance"],
                    "type": "web",
                    "content_preview": result["content"][:100]
                })

        # æ·»åŠ  AI åˆ†æä¾†æº
        all_sources.append({
            "source": "Ollama AI ç¶œåˆåˆ†æ",
            "relevance": 0.95,
            "type": "ai",
            "model": PREFERRED_MODEL
        })

        total_time = (datetime.now() - start_time).total_seconds()

        return QuestionResponse(
            answer=integrated_answer,
            sources=all_sources,
            metadata={
                "type": "hybrid",
                "model_used": PREFERRED_MODEL or "integration",
                "ollama_available": OLLAMA_AVAILABLE,
                "processing_time": round(total_time, 2),
                "llm_time": round(llm_time, 2) if llm_time else 0,
                "rag_status": rag_results["status"],
                "rag_results_count": len(rag_results["results"]),
                "web_status": web_results["status"],
                "web_results_count": len(web_results["results"]),
                "search_engine": web_results.get("search_engine", "simulated"),
                "integration_method": "ollama_ai_integration",
                "sources_used": {
                    "rag": rag_has_content,
                    "web": web_has_content,
                    "ai": True
                }
            }
        )

    except Exception as e:
        error_time = (datetime.now() - start_time).total_seconds()
        return QuestionResponse(
            answer=f"æ··åˆå•ç­”å¤±æ•—ï¼š{str(e)[:100]}",
            sources=[],
            metadata={
                "type": "hybrid",
                "error": str(e),
                "processing_time": round(error_time, 2)
            }
        )

@router.get("/status")
async def get_qa_status():
    """ç²å–å•ç­”ç³»çµ±ç‹€æ…‹"""
    return {
        "status": "running",
        "service": "qa_system",
        "version": "1.0.0",
        "ollama": {
            "available": OLLAMA_AVAILABLE,
            "host": OLLAMA_HOST,
            "preferred_model": PREFERRED_MODEL,
            "available_models": [m.get('name', '') for m in AVAILABLE_MODELS]
        },
        "rag": {
            "available": RAG_VECTORDB is not None,
            "db_dir": DB_DIR,
            "embedding_model": EMBEDDING_MODEL,
            "document_count": RAG_VECTORDB._collection.count() if RAG_VECTORDB else 0
        },
        "search": {
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None,
            "search_tool": "DuckDuckGoSearchRun"
        },
        "capabilities": {
            "rag_enabled": RAG_VECTORDB is not None,
            "web_search_enabled": DUCKDUCKGO_SEARCH is not None,
            "hybrid_qa_enabled": (RAG_VECTORDB is not None) or (DUCKDUCKGO_SEARCH is not None),
            "ai_powered": OLLAMA_AVAILABLE
        },
        "timestamp": datetime.now().isoformat()
    }

@router.get("/test")
async def test_endpoint():
    """æ¸¬è©¦ç«¯é»"""
    return {
        "status": "ok",
        "message": "QA ç³»çµ±å·¥ä½œæ­£å¸¸",
        "ollama_status": "connected" if OLLAMA_AVAILABLE else "disconnected",
        "rag_status": "connected" if RAG_VECTORDB else "disconnected",
        "duckduckgo_status": "connected" if DUCKDUCKGO_SEARCH else "disconnected",
        "preferred_model": PREFERRED_MODEL,
        "test_suggestion": "è«‹å˜—è©¦ POST /api/qa/hybrid é€²è¡Œç¶œåˆå•ç­”æ¸¬è©¦"
    }

@router.get("/search-test")
async def test_search():
    """æ¸¬è©¦æœå°‹åŠŸèƒ½"""
    try:
        search_results = await search_duckduckgo("å°ç£å¤©æ°£", max_results=3)
        return {
            "status": "ok",
            "search_test": "completed",
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None,
            "search_results": search_results
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None
        }

@router.get("/rag-test")
async def test_rag():
    """æ¸¬è©¦ RAG åŠŸèƒ½"""
    try:
        rag_results = await search_rag("æ¸¬è©¦", k=2)
        return {
            "status": "ok",
            "rag_test": "completed",
            "rag_available": RAG_VECTORDB is not None,
            "rag_results": rag_results,
            "document_count": RAG_VECTORDB._collection.count() if RAG_VECTORDB else 0
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "rag_available": RAG_VECTORDB is not None
        }

@router.get("/models")
async def get_available_models():
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return {
        "available_models": [m.get('name', '') for m in AVAILABLE_MODELS],
        "preferred_model": PREFERRED_MODEL,
        "ollama_host": OLLAMA_HOST,
        "ollama_status": "connected" if OLLAMA_AVAILABLE else "disconnected"
    }

@router.post("/rag", response_model=QuestionResponse)
async def rag_qa(request: QuestionRequest):
    """RAG å•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await rag_qa_internal(request.question.strip())

@router.post("/web", response_model=QuestionResponse)
async def web_qa(request: QuestionRequest):
    """ç¶²è·¯å•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await web_qa_internal(request.question.strip())

@router.post("/hybrid", response_model=QuestionResponse)
async def hybrid_qa(request: QuestionRequest):
    """æ··åˆå•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await hybrid_qa_internal(request.question.strip())
