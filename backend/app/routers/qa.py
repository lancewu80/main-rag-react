from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import asyncio
import os
import json
from datetime import datetime
import requests
import sys
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.tools import DuckDuckGoSearchRun

import ctypes
import os
import time
import json

from typing import Optional, Tuple

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# åŠ è¼‰ PyTorch æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ PyTorch æ­£åœ¨ä½¿ç”¨è¨­å‚™: {device}")
rerank_model_name = "BAAI/bge-reranker-base"
rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)
rerank_model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)
rerank_model.eval() # è¨­å®šç‚ºæ¨è«–æ¨¡å¼

def torch_rerank(query, documents, top_n=3):
    if not documents:
        return []

    # ç¢ºä¿æ¨¡å‹åœ¨æ­£ç¢ºçš„è¨­å‚™ (GPU/CPU)
    rerank_model.to(device)

    pairs = [[query, doc.page_content] for doc in documents]

    with torch.no_grad():
        # å°‡æ•¸æ“šç§»è‡³è¨­å‚™
        inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)

        # æ¨¡å‹æ¨è«–
        logits = rerank_model(**inputs).logits
        scores = logits.view(-1,).float()

        # æ’åºä¸¦å–å‡ºå‰ top_n å
        scored_pairs = zip(scores.cpu().tolist(), documents)
        sorted_docs = sorted(scored_pairs, key=lambda x: x[0], reverse=True)

        # é€™è£¡æœƒç”¨åˆ°å‚³å…¥çš„ top_n
        return [doc for score, doc in sorted_docs[:top_n]]

# --- C å‡½å¼åº«åˆå§‹åŒ– ---
# å–å¾—ç›®å‰æª”æ¡ˆçš„çµ•å°è·¯å¾‘ï¼Œä¸¦æŒ‡å‘ ../c/io_writer.dll
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DLL_PATH = os.path.join(os.path.dirname(CURRENT_DIR), "c", "io_writer.dll")

try:
    c_lib = ctypes.CDLL(DLL_PATH)
    c_lib.fast_write.argtypes = [ctypes.c_char_p, ctypes.c_char_p]
    c_lib.fast_write.restype = ctypes.c_double
    print(f"âœ… æˆåŠŸè¼‰å…¥ C æ“´å±•: {DLL_PATH}")
except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥ C æ“´å±•: {e}")
    c_lib = None

router = APIRouter()

# ============ å®‰å…¨è™•ç†å‡½æ•¸ ============
def safe_round(value, decimals=2):
    """å®‰å…¨åœ°é€²è¡Œå››æ¨äº”å…¥"""
    if value is None:
        return 0.0
    try:
        return round(float(value), decimals)
    except (ValueError, TypeError):
        return 0.0

def safe_float(value):
    """å®‰å…¨åœ°è½‰æ›ç‚º float"""
    if value is None:
        return 0.0
    try:
        return float(value)
    except (ValueError, TypeError):
        return 0.0
# =====================================

# é…ç½® - ä¿®æ­£ï¼šçµ±ä¸€ä½¿ç”¨çµ•å°è·¯å¾‘
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DB_DIR = os.path.join(BASE_DIR, "vectordb")  # æ”¹æˆçµ•å°è·¯å¾‘
DOCS_DIR = os.path.join(BASE_DIR, "docs")    # æ”¹æˆçµ•å°è·¯å¾‘
OLLAMA_HOST = "http://localhost:11434"  # Ollama é è¨­åœ°å€

print(f"QAç³»çµ±é…ç½®:")
print(f"BASE_DIR: {BASE_DIR}")
print(f"DB_DIR: {DB_DIR}")
print(f"DOCS_DIR: {DOCS_DIR}")

# RAG é…ç½®
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
CHUNK_SIZE = 400
CHUNK_OVERLAP = 100

# åˆå§‹åŒ– DuckDuckGo æœå°‹
def init_duckduckgo_search():
    """åˆå§‹åŒ– DuckDuckGo æœå°‹å·¥å…·"""
    try:
        search_tool = DuckDuckGoSearchRun()
        # æ¸¬è©¦æœå°‹
        test_result = search_tool.run("test")[:100]
        print(f"âœ… DuckDuckGo æœå°‹å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        return search_tool
    except Exception as e:
        print(f"âŒ DuckDuckGo æœå°‹å·¥å…·åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

# åˆå§‹åŒ– RAG å‘é‡è³‡æ–™åº«
def init_rag_vector_db():
    """åˆå§‹åŒ– RAG å‘é‡è³‡æ–™åº«"""
    try:
        if not os.path.exists(DB_DIR):
            print(f"âŒ å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {DB_DIR}")
            return None

        # è¼‰å…¥åµŒå…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        vectordb = Chroma(
            persist_directory=DB_DIR,
            embedding_function=embeddings,
            collection_name="default"
        )

        # æ¸¬è©¦è³‡æ–™åº«
        count = vectordb._collection.count()
        print(f"âœ… RAG å‘é‡è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸ")
        print(f"   è³‡æ–™åº«è·¯å¾‘: {DB_DIR}")
        print(f"   æ–‡ä»¶æ•¸é‡: {count}")

        if count == 0:
            print(f"âš ï¸  è­¦å‘Š: å‘é‡è³‡æ–™åº«ç‚ºç©ºï¼Œè«‹å…ˆä½¿ç”¨ knowledge API å»ºç½®çŸ¥è­˜åº«")

        return vectordb
    except Exception as e:
        print(f"âŒ RAG å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

# åˆå§‹åŒ–æœå°‹å·¥å…·å’Œå‘é‡è³‡æ–™åº«
DUCKDUCKGO_SEARCH = init_duckduckgo_search()
RAG_VECTORDB = init_rag_vector_db()

class QuestionRequest(BaseModel):
    question: str
    type: str = "rag"  # rag, web, hybrid
    options: Optional[Dict[str, Any]] = None

class QuestionResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]] = []
    metadata: Dict[str, Any] = {}

# æª¢æŸ¥ Ollama æ˜¯å¦å¯ç”¨
def check_ollama_available():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama é€£æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {[m['name'] for m in models]}")
            return True, models
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
        print(f"   è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ: ollama serve")
    return False, []

# åˆå§‹åŒ–æ™‚æª¢æŸ¥ Ollama
OLLAMA_AVAILABLE, AVAILABLE_MODELS = check_ollama_available()

# é è¨­ä½¿ç”¨ DeepSeek R1ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å…¶ä»–å¯ç”¨æ¨¡å‹
def get_preferred_model():
    """ç²å–é¦–é¸æ¨¡å‹"""
    preferred_models = [
        "deepseek-r1:8b",
        "qwen2.5:7b",
        "llama3.2:3b",
        "mistral:7b",
        "gemma:2b"
    ]

    for model in preferred_models:
        for available in AVAILABLE_MODELS:
            if model in available.get('name', ''):
                print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model}")
                return model

    if AVAILABLE_MODELS:
        first_model = AVAILABLE_MODELS[0].get('name', '')
        print(f"âœ… ä½¿ç”¨å¯ç”¨æ¨¡å‹: {first_model}")
        return first_model

    print("âš ï¸  æ²’æœ‰å¯ç”¨çš„ Ollama æ¨¡å‹")
    return None

PREFERRED_MODEL = get_preferred_model()

async def call_ollama_api(
    prompt: str,
    model: Optional[str] = None
) -> Tuple[str, float]:
    """èª¿ç”¨ Ollama API ç”Ÿæˆå›ç­”"""

    if not OLLAMA_AVAILABLE or not PREFERRED_MODEL:
        return "âš ï¸ Ollama æœå‹™æœªé€£æ¥ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚", 0.0

    model_to_use: str = model or PREFERRED_MODEL

    try:
        payload = {
            "model": model_to_use,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 2000
            }
        }

        start_time = datetime.now()
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=payload,
            timeout=300
        )

        if response.status_code == 200:
            result = response.json()
            processing_time = (datetime.now() - start_time).total_seconds()

            answer = result.get("response", "").strip()

            if answer.startswith("ã€‚"):
                answer = answer[1:]
            if answer.startswith("ï¼Œ"):
                answer = answer[1:]

            print(f"âœ… Ollama å›ç­”ç”ŸæˆæˆåŠŸï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            return answer, processing_time
        else:
            return f"âŒ Ollama API éŒ¯èª¤: {response.status_code}", 0.0

    except requests.exceptions.Timeout:
        return "âŒ Ollama è«‹æ±‚è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", 0.0
    except Exception as e:
        return f"âŒ Ollama èª¿ç”¨å¤±æ•—: {str(e)}", 0.0


# DuckDuckGo æœå°‹å‡½æ•¸
async def search_duckduckgo(query: str, max_results: int = 5) -> Dict[str, Any]:
    """ä½¿ç”¨ DuckDuckGo é€²è¡Œæœå°‹"""
    if not DUCKDUCKGO_SEARCH:
        return {
            "status": "error",
            "message": "DuckDuckGo æœå°‹å·¥å…·æœªåˆå§‹åŒ–",
            "results": [],
            "query": query
        }

    try:
        print(f"ğŸ” DuckDuckGo æœå°‹: {query}")

        # åŸ·è¡Œæœå°‹
        search_result = DUCKDUCKGO_SEARCH.run(query)

        # è§£ææœå°‹çµæœï¼ˆDuckDuckGoSearchRun è¿”å›çš„æ˜¯æ–‡æœ¬ï¼Œéœ€è¦è§£æï¼‰
        results = []

        if search_result:
            # å°‡æœå°‹çµæœåˆ†å‰²æˆæ®µè½
            paragraphs = search_result.split('\n\n')
            for i, paragraph in enumerate(paragraphs[:max_results]):
                if paragraph.strip():
                    results.append({
                        "index": i + 1,
                        "content": paragraph.strip()[:500],  # é™åˆ¶é•·åº¦
                        "relevance": 1.0 - (i * 0.1),  # ç°¡å–®ç›¸é—œæ€§è©•åˆ†
                        "type": "web_search"
                    })

        print(f"âœ… æ‰¾åˆ° {len(results)} å€‹æœå°‹çµæœ")

        return {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœ",
            "results": results,
            "query": query,
            "search_engine": "DuckDuckGo"
        }

    except Exception as e:
        print(f"âŒ DuckDuckGo æœå°‹å¤±æ•—: {e}")
        return {
            "status": "error",
            "message": str(e),
            "results": [],
            "query": query
        }

# RAG æª¢ç´¢å‡½æ•¸
async def search_rag(query: str, k: int = 4) -> Dict[str, Any]:
    """ä½¿ç”¨ RAG æª¢ç´¢æœ¬åœ°çŸ¥è­˜åº«"""
    if not RAG_VECTORDB:
        return {
            "status": "error",
            "message": "RAG å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–",
            "results": [],
            "query": query
        }

    try:
        print(f"ğŸ“š RAG æª¢ç´¢: {query}")

        # åŸ·è¡Œç›¸ä¼¼åº¦æœå°‹
        docs = RAG_VECTORDB.similarity_search(query, k=k)

        results = []
        for i, doc in enumerate(docs):
            content = doc.page_content[:400]  # é™åˆ¶é•·åº¦
            metadata = doc.metadata

            results.append({
                "index": i + 1,
                "content": content,
                "source": metadata.get("source", "æœªçŸ¥ä¾†æº"),
                "relevance": 1.0 - (i * 0.15),  # ç°¡å–®ç›¸é—œæ€§è©•åˆ†
                "type": "rag"
            })

        print(f"âœ… æ‰¾åˆ° {len(results)} å€‹ç›¸é—œæ–‡æª”")

        return {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœ",
            "results": results,
            "query": query
        }

    except Exception as e:
        print(f"âŒ RAG æª¢ç´¢å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": str(e),
            "results": [],
            "query": query
        }

# RAG å•ç­” (å·²æ•´åˆ PyTorch Rerank å„ªåŒ–ç‰ˆ)
async def rag_qa_internal(question: str) -> QuestionResponse:
    """åŸ·è¡Œ RAG å•ç­”æµç¨‹ï¼Œä¸¦é€é PyTorch é€²è¡Œé‡æ’å„ªåŒ–"""
    try:
        start_time = datetime.now()

        # 1. åˆå§‹æª¢ç´¢ï¼šæ“´å¤§ç¯„åœè‡³ k=10ï¼Œè®“ Reranker æœ‰æŒ‘é¸ç©ºé–“
        rag_results = await search_rag(question, k=10)

        if rag_results["status"] == "error":
            raise Exception(rag_results["message"])

        if not rag_results["results"]:
            return QuestionResponse(
                answer="âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šã€‚",
                sources=[],
                metadata={
                    "type": "rag",
                    "processing_time": round((datetime.now() - start_time).total_seconds(), 2),
                    "message": "çŸ¥è­˜åº«ä¸­æ²’æœ‰ç›¸é—œå…§å®¹"
                }
            )

        # 2. PyTorch é‡æ’é‚è¼¯
        # å°‡ search_rag çš„çµæœè½‰æ›ç‚º torch_rerank éœ€è¦çš„ Document æ ¼å¼ç‰©ä»¶
        class SimpleDoc:
            def __init__(self, content, metadata):
                self.page_content = content
                self.metadata = metadata

        initial_docs = [SimpleDoc(r['content'], {'source': r['source'], 'relevance': r['relevance']}) for r in rag_results["results"]]

        # èª¿ç”¨æ‚¨å®šç¾©çš„ PyTorch Rerank å‡½æ•¸ (å–å‡ºåˆ†æ•¸æœ€é«˜çš„å‰ 4 å)
        # [è¨»] æ­¤è™•æœƒä½¿ç”¨æ‚¨å‰›å¯«å¥½çš„ torch_rerankï¼Œä¸”å…§éƒ¨å·²è™•ç† sorted_docs å®šç¾©å•é¡Œ
        rerank_start = time.time()
        final_docs = torch_rerank(question, initial_docs, top_n=4)
        rerank_time = time.time() - rerank_start

        # 3. æ§‹å»ºä¸Šä¸‹æ–‡ (ä½¿ç”¨é‡æ’å¾Œçš„ç²¾é¸å…§å®¹)
        context = "ã€æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Š (å·²é€šé PyTorch Rerank å„ªåŒ–)ã€‘\n\n"
        for i, doc in enumerate(final_docs):
            context += f"ä¾†æº: {doc.metadata['source']}\n"
            context += f"å…§å®¹: {doc.page_content}\n\n"

        # ç”Ÿæˆæç¤ºè©
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹æä¾›ï¼š
1. æ ¸å¿ƒè³‡è¨Šæ‘˜è¦
2. å…·é«”ç´°ç¯€å’Œæ•¸æ“š
3. å¯¦ç”¨å»ºè­°ï¼ˆå¦‚é©ç”¨ï¼‰

ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¿æŒå°ˆæ¥­ã€å®¢è§€ä¸”æ˜“æ–¼ç†è§£ã€‚è¨»æ˜è³‡è¨Šä¾†æºç‚ºæœ¬åœ°çŸ¥è­˜åº«ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama ç”Ÿæˆå›ç­”
        print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­” (æ¨¡å‹: {PREFERRED_MODEL})...")
        answer, llm_time = await call_ollama_api(prompt)

        # æº–å‚™ä¾†æºè³‡è¨Š (åæ˜ é‡æ’å¾Œçš„é †åº)
        sources = []
        for doc in final_docs:
            sources.append({
                "source": f"æœ¬åœ°çŸ¥è­˜åº«: {doc.metadata['source']}",
                "relevance": "High (Reranked)",
                "type": "rag",
                "content_preview": doc.page_content[:100]
            })

        total_time = (datetime.now() - start_time).total_seconds()

        return QuestionResponse(
            answer=answer,
            sources=sources,
            metadata={
                "type": "rag_reranked",
                "model_used": PREFERRED_MODEL or "unknown",
                "ollama_available": OLLAMA_AVAILABLE,
                "processing_time": round(total_time, 2),
                "llm_time": round(llm_time, 2) if llm_time else 0,
                "rerank_time": round(rerank_time, 4),
                "results_count": len(final_docs),
                "device": str(device) # é¡¯ç¤ºæ˜¯ç”¨ CPU é‚„æ˜¯ CUDA
            }
        )

    except Exception as e:
        error_time = (datetime.now() - start_time).total_seconds()
        return QuestionResponse(
            answer=f"RAG å•ç­”å¤±æ•—ï¼š{str(e)}",
            sources=[],
            metadata={
                "type": "rag",
                "error": str(e),
                "processing_time": round(error_time, 2)
            }
        )

# ç¶²è·¯å•ç­”
async def web_qa_internal(question: str) -> QuestionResponse:
    """åŸ·è¡Œç¶²è·¯å•ç­”æµç¨‹"""
    try:
        start_time = datetime.now()

        # ç¶²è·¯æœå°‹
        web_results = await search_duckduckgo(question, max_results=5)

        if web_results["status"] == "error":
            raise Exception(web_results["message"])

        if not web_results["results"]:
            return QuestionResponse(
                answer="âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ç¶²è·¯æœå°‹çµæœã€‚",
                sources=[],
                metadata={
                    "type": "web",
                    "processing_time": round((datetime.now() - start_time).total_seconds(), 2),
                    "message": "ç¶²è·¯æœå°‹æ²’æœ‰çµæœ"
                }
            )

        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = "ã€ç¶²è·¯æœå°‹è³‡è¨Šã€‘\n\n"
        for i, result in enumerate(web_results["results"], 1):
            context += f"çµæœ {i} (ç›¸é—œæ€§: {result['relevance']:.2f}):\n"
            context += f"{result['content']}\n\n"

        # ç”Ÿæˆæç¤ºè©
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¶²è·¯æœå°‹çµæœå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹æä¾›ï¼š
1. æ ¸å¿ƒè³‡è¨Šæ‘˜è¦
2. æœ€æ–°å‹•æ…‹å’Œè¶¨å‹¢
3. å¯¦ç”¨å»ºè­°

ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ³¨æ„è³‡è¨Šçš„æ™‚æ•ˆæ€§ã€‚è¨»æ˜è³‡è¨Šä¾†æºç‚ºç¶²è·¯æœå°‹ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama ç”Ÿæˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        answer, llm_time = await call_ollama_api(prompt)

        # æº–å‚™ä¾†æºè³‡è¨Š
        sources = []
        for result in web_results["results"]:
            sources.append({
                "source": f"DuckDuckGo æœå°‹çµæœ #{result['index']}",
                "relevance": result["relevance"],
                "type": "web",
                "content_preview": result["content"][:100]
            })

        total_time = (datetime.now() - start_time).total_seconds()

        return QuestionResponse(
            answer=answer,
            sources=sources,
            metadata={
                "type": "web",
                "model_used": PREFERRED_MODEL or "unknown",
                "ollama_available": OLLAMA_AVAILABLE,
                "processing_time": round(total_time, 2),
                "llm_time": round(llm_time, 2) if llm_time else 0,
                "search_engine": web_results.get("search_engine", "DuckDuckGo"),
                "results_count": len(web_results["results"])
            }
        )

    except Exception as e:
        error_time = (datetime.now() - start_time).total_seconds()
        return QuestionResponse(
            answer=f"ç¶²è·¯å•ç­”å¤±æ•—ï¼š{str(e)}",
            sources=[],
            metadata={
                "type": "web",
                "error": str(e),
                "processing_time": round(error_time, 2)
            }
        )

# æ··åˆå•ç­”
async def hybrid_qa_internal(question: str) -> QuestionResponse:
    """åŸ·è¡Œæ··åˆå•ç­”æµç¨‹ï¼ˆRAG + Web Searchï¼‰"""
    try:
        start_time = datetime.now()

        # ä¸¦è¡ŒåŸ·è¡Œ RAG æª¢ç´¢å’Œç¶²è·¯æœå°‹
        print("ğŸ”„ æ­£åœ¨åŸ·è¡Œæ··åˆæª¢ç´¢...")
        rag_results, web_results = await asyncio.gather(
            search_rag(question, k=4),
            search_duckduckgo(question, max_results=5)
        )

        print(f"RAG ç‹€æ…‹: {rag_results['status']}, çµæœæ•¸: {len(rag_results['results'])}")
        print(f"Web ç‹€æ…‹: {web_results['status']}, çµæœæ•¸: {len(web_results['results'])}")

        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„çµæœ
        rag_has_content = rag_results["status"] == "success" and len(rag_results["results"]) > 0
        web_has_content = web_results["status"] == "success" and len(web_results["results"]) > 0

# æº–å‚™è¦å¯«å…¥çš„æ•¸æ“š
        log_data = json.dumps({
            "question": question,
            "rag": rag_results,
            "web": web_results,
            "timestamp": datetime.now().isoformat()
        }, ensure_ascii=False)
        data_bytes = log_data.encode('utf-8')

        # 1. Python å¯«å…¥æ¸¬è©¦
        py_start = time.perf_counter()
        with open("perf_python.json", "w", encoding="utf-8") as f:
            f.write(log_data*10000)
        py_duration = time.perf_counter() - py_start

# --- 2. C å¯«å…¥æ¸¬è©¦ (ä¿®æ­£è®Šæ•¸ç¯„åœå•é¡Œ) ---
# ä¿®æ­£å¾Œçš„ C å¯«å…¥æ¸¬è©¦
        c_duration = -1.0
        filename = "perf_c.json"
        target_path = os.path.join(BASE_DIR, filename)
        abs_target_path = os.path.normpath(os.path.abspath(target_path))

        if c_lib:
            try:
                # ğŸ’¡ å˜—è©¦å°‡è·¯å¾‘è½‰ç‚º Windows ç³»çµ±åŸç”Ÿç·¨ç¢¼ (é‡è¦ï¼)
                # å¦‚æœ utf-8 æœƒå ± Errno 22 (ç„¡æ•ˆåƒæ•¸)ï¼Œè«‹æ”¹ç”¨ 'mbcs'
                try:
                    c_path_bytes = abs_target_path.encode('mbcs')
                except:
                    c_path_bytes = abs_target_path.encode('utf-8')

                # ç¢ºä¿ data_bytes ä¹Ÿæ˜¯æ­£ç¢ºçš„ bytes
                if isinstance(log_data, str):
                    data_bytes = (log_data*10000).encode('utf-8')
                else:
                    data_bytes = log_data*10000

                c_duration = c_lib.fast_write(c_path_bytes, data_bytes)
            except Exception as e:
                print(f"âŒ å‘¼å« C DLL æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")

        # --- 3. é¡¯ç¤ºçµæœ ---
        print(f"--- I/O Performance Analysis ---")
        print(f"Target Path:  {abs_target_path}")
        print(f"Python Write: {py_duration:.6f} s")
        print(f"C Write:      {c_duration:.6f} s")

        if c_duration == -1.0:
            print(f"âŒ éŒ¯èª¤æç¤º: C èªè¨€ç„¡æ³•é–‹å•Ÿæª”æ¡ˆã€‚åŸå› å¯èƒ½æ˜¯æ¬Šé™ä¸è¶³ã€è·¯å¾‘éŒ¯èª¤æˆ– DLL è¼‰å…¥å¤±æ•—ã€‚")
        elif c_duration > 0:
            print(f"Speedup:      {py_duration / (c_duration if c_duration > 0 else 0.000001):.2f}x")
        print(f"--------------------------------")

        # æ§‹å»ºæ•´åˆçš„ä¸Šä¸‹æ–‡
        context_parts = []

        if rag_has_content:
            rag_context = "ã€æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šã€‘\n\n"
            for result in rag_results["results"]:
                rag_context += f"ä¾†æº: {result['source']}\n"
                rag_context += f"ç›¸é—œæ€§: {result['relevance']:.2f}\n"
                rag_context += f"å…§å®¹: {result['content']}\n\n"
            context_parts.append(rag_context)

        if web_has_content:
            web_context = "ã€ç¶²è·¯æœå°‹è³‡è¨Šã€‘\n\n"
            for i, result in enumerate(web_results["results"], 1):
                web_context += f"çµæœ {i} (ç›¸é—œæ€§: {result['relevance']:.2f}):\n"
                web_context += f"{result['content']}\n\n"
            context_parts.append(web_context)

        if not rag_has_content and not web_has_content:
            context = "âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„æœ¬åœ°çŸ¥è­˜åº«æˆ–ç¶²è·¯è³‡è¨Šã€‚"
        else:
            context = "\n".join(context_parts)

        # æ ¹æ“šå¯ç”¨çš„è³‡è¨Šé¡å‹æ§‹å»ºä¸åŒçš„æç¤ºè©
        if rag_has_content and web_has_content:
            prompt = f"""è«‹ç¶œåˆä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹çš„è³‡è¨Šï¼Œæä¾›ä¸€å€‹å…¨é¢ã€æº–ç¢ºçš„å›ç­”ï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šæä¾›ï¼š
1. æ ¸å¿ƒè³‡è¨Šæ‘˜è¦ï¼ˆç¶œåˆå…©æ–¹é¢è³‡è¨Šï¼‰
2. å…·é«”ç´°ç¯€å’Œæ•¸æ“šï¼ˆå„ªå…ˆä½¿ç”¨æœ¬åœ°çŸ¥è­˜åº«çš„æ¬Šå¨è³‡è¨Šï¼‰
3. å¯¦ç”¨å»ºè­°å’Œæ“ä½œæ­¥é©Ÿ
4. æ³¨æ„äº‹é …å’Œé¢¨éšªæç¤º

å¦‚æœè³‡è¨Šæœ‰è¡çªï¼š
- æŠ€è¡“æ€§ã€å°ˆæ¥­æ€§è³‡è¨Šä»¥æœ¬åœ°çŸ¥è­˜åº«ç‚ºæº–
- æ™‚æ•ˆæ€§ã€æ–°èæ€§è³‡è¨Šä»¥ç¶²è·¯æœå°‹ç‚ºæº–
- è¨»æ˜è³‡è¨Šä¾†æºï¼ˆæœ¬åœ°çŸ¥è­˜åº«/ç¶²è·¯æœå°‹ï¼‰

ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¿æŒå°ˆæ¥­ã€å®¢è§€ä¸”æ˜“æ–¼ç†è§£ã€‚

ç¶œåˆå›ç­”ï¼š"""

        elif rag_has_content:
            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹ä¸»è¦ä½¿ç”¨æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”ï¼Œå¦‚æœè³‡è¨Šä¸è¶³å¯ä»¥è£œå……æ‚¨çš„é€šç”¨çŸ¥è­˜ã€‚
è¨»æ˜è³‡è¨Šä¾†æºç‚ºæœ¬åœ°çŸ¥è­˜åº«ã€‚

å›ç­”ï¼š"""

        elif web_has_content:
            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¶²è·¯æœå°‹çµæœå›ç­”å•é¡Œï¼š

{context}

ã€å•é¡Œã€‘
{question}

è«‹ä¸»è¦ä½¿ç”¨ç¶²è·¯æœå°‹çµæœå›ç­”ï¼Œæ³¨æ„è³‡è¨Šçš„æ™‚æ•ˆæ€§ã€‚
è¨»æ˜è³‡è¨Šä¾†æºç‚ºç¶²è·¯æœå°‹ã€‚

å›ç­”ï¼š"""

        else:
            prompt = f"""è«‹æ ¹æ“šæ‚¨çš„çŸ¥è­˜å›ç­”ä»¥ä¸‹å•é¡Œï¼š

ã€å•é¡Œã€‘
{question}

è«‹æä¾›æº–ç¢ºã€æœ‰ç”¨çš„è³‡è¨Šï¼Œä¸¦è¨»æ˜é€™æ˜¯åŸºæ–¼é€šç”¨çŸ¥è­˜çš„å›ç­”ã€‚

å›ç­”ï¼š"""

        # èª¿ç”¨ Ollama ç”Ÿæˆæ•´åˆå›ç­”
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç¶œåˆå›ç­”...")
        integrated_answer, llm_time = await call_ollama_api(prompt)

        # åˆä½µä¾†æºè³‡è¨Š
        all_sources = []

        if rag_has_content:
            for result in rag_results["results"][:3]:  # åªå–å‰3å€‹
                all_sources.append({
                    "source": f"æœ¬åœ°çŸ¥è­˜åº«: {result['source']}",
                    "relevance": result["relevance"],
                    "type": "rag",
                    "content_preview": result["content"][:100]
                })

        if web_has_content:
            for result in web_results["results"][:5]:  # åªå–å‰5å€‹
                all_sources.append({
                    "source": f"DuckDuckGo æœå°‹çµæœ #{result['index']}",
                    "relevance": result["relevance"],
                    "type": "web",
                    "content_preview": result["content"][:100]
                })

        # æ·»åŠ  AI åˆ†æä¾†æº
        all_sources.append({
            "source": "Ollama AI ç¶œåˆåˆ†æ",
            "relevance": 0.95,
            "type": "ai",
            "model": PREFERRED_MODEL
        })

        total_time = (datetime.now() - start_time).total_seconds()

        return QuestionResponse(
            answer=integrated_answer,
            sources=all_sources,
            metadata={
                "type": "hybrid",
                "model_used": PREFERRED_MODEL or "integration",
                "ollama_available": OLLAMA_AVAILABLE,
                "processing_time": round(total_time, 2),
                "llm_time": round(llm_time, 2) if llm_time else 0,
                "rag_status": rag_results["status"],
                "rag_results_count": len(rag_results["results"]),
                "web_status": web_results["status"],
                "web_results_count": len(web_results["results"]),
                "search_engine": web_results.get("search_engine", "simulated"),
                "integration_method": "ollama_ai_integration",
                "sources_used": {
                    "rag": rag_has_content,
                    "web": web_has_content,
                    "ai": True
                }
            }
        )

    except Exception as e:
        error_time = (datetime.now() - start_time).total_seconds()
        return QuestionResponse(
            answer=f"æ··åˆå•ç­”å¤±æ•—ï¼š{str(e)[:100]}",
            sources=[],
            metadata={
                "type": "hybrid",
                "error": str(e),
                "processing_time": round(error_time, 2)
            }
        )

@router.get("/status")
async def get_qa_status():
    """ç²å–å•ç­”ç³»çµ±ç‹€æ…‹"""
    return {
        "status": "running",
        "service": "qa_system",
        "version": "1.0.0",
        "ollama": {
            "available": OLLAMA_AVAILABLE,
            "host": OLLAMA_HOST,
            "preferred_model": PREFERRED_MODEL,
            "available_models": [m.get('name', '') for m in AVAILABLE_MODELS]
        },
        "rag": {
            "available": RAG_VECTORDB is not None,
            "db_dir": DB_DIR,
            "embedding_model": EMBEDDING_MODEL,
            "document_count": RAG_VECTORDB._collection.count() if RAG_VECTORDB else 0
        },
        "search": {
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None,
            "search_tool": "DuckDuckGoSearchRun"
        },
        "capabilities": {
            "rag_enabled": RAG_VECTORDB is not None,
            "web_search_enabled": DUCKDUCKGO_SEARCH is not None,
            "hybrid_qa_enabled": (RAG_VECTORDB is not None) or (DUCKDUCKGO_SEARCH is not None),
            "ai_powered": OLLAMA_AVAILABLE
        },
        "timestamp": datetime.now().isoformat()
    }

@router.get("/test")
async def test_endpoint():
    """æ¸¬è©¦ç«¯é»"""
    return {
        "status": "ok",
        "message": "QA ç³»çµ±å·¥ä½œæ­£å¸¸",
        "ollama_status": "connected" if OLLAMA_AVAILABLE else "disconnected",
        "rag_status": "connected" if RAG_VECTORDB else "disconnected",
        "duckduckgo_status": "connected" if DUCKDUCKGO_SEARCH else "disconnected",
        "preferred_model": PREFERRED_MODEL,
        "test_suggestion": "è«‹å˜—è©¦ POST /api/qa/hybrid é€²è¡Œç¶œåˆå•ç­”æ¸¬è©¦"
    }

@router.get("/search-test")
async def test_search():
    """æ¸¬è©¦æœå°‹åŠŸèƒ½"""
    try:
        search_results = await search_duckduckgo("å°ç£å¤©æ°£", max_results=3)
        return {
            "status": "ok",
            "search_test": "completed",
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None,
            "search_results": search_results
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "duckduckgo_available": DUCKDUCKGO_SEARCH is not None
        }

@router.get("/rag-test")
async def test_rag():
    """æ¸¬è©¦ RAG åŠŸèƒ½"""
    try:
        rag_results = await search_rag("æ¸¬è©¦", k=2)
        return {
            "status": "ok",
            "rag_test": "completed",
            "rag_available": RAG_VECTORDB is not None,
            "rag_results": rag_results,
            "document_count": RAG_VECTORDB._collection.count() if RAG_VECTORDB else 0
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "rag_available": RAG_VECTORDB is not None
        }

@router.get("/models")
async def get_available_models():
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return {
        "available_models": [m.get('name', '') for m in AVAILABLE_MODELS],
        "preferred_model": PREFERRED_MODEL,
        "ollama_host": OLLAMA_HOST,
        "ollama_status": "connected" if OLLAMA_AVAILABLE else "disconnected"
    }

@router.post("/rag", response_model=QuestionResponse)
async def rag_qa(request: QuestionRequest):
    """RAG å•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await rag_qa_internal(request.question.strip())

@router.post("/web", response_model=QuestionResponse)
async def web_qa(request: QuestionRequest):
    """ç¶²è·¯å•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await web_qa_internal(request.question.strip())

@router.post("/hybrid", response_model=QuestionResponse)
async def hybrid_qa(request: QuestionRequest):
    """æ··åˆå•ç­”ç«¯é»"""
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="å•é¡Œä¸èƒ½ç‚ºç©º")
    return await hybrid_qa_internal(request.question.strip())
